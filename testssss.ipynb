{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import re\n",
    "\n",
    "from newdataset import BilingualDataset, causal_mask\n",
    "\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size:  257991\n",
      "Maximum sequence length of Feynman diagram        : 27\n",
      "Maximum sequence length of squared amplitudes: 137\n",
      "Example sqamp: [ m_e^6 , m_e^4 , m_e^2 , 1 ] , [ -32 , 8 *( 4*s_13 +3*s_14 +2*s_25 -4*s_34) , -8 *( s_11*s_14 -2* s_11*s_34 +3* s_12*s_45 +2* s_13*s_25 +2* s_13*s_34 +3* s_15*s_24 -4* s_23*s_45 -4* s_24*s_35) , 8 *( s_11* s_12*s_45 +s_11* s_15*s_24 -2* s_11*s_23*s_45 -2* s_11*s_24*s_35 +2* s_13*s_23*s_45 +2* s_13*s_24*s_35) ] , [ ( m_e^2 -s_11 +2*s_13)^2 *( s_22 -2*s_25 +s_55)^2 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def prepro_feyn(data):\n",
    "\n",
    "    for r in (('(', '('), (')', ')'), ('  ', ' '), (' e(', 'e(m_e,-1,' ),(' mu(', 'mu(m_mu,-1,') ,(' u(', ' u(m_u,2/3,'), (' d(', 'd(m_d,-1/3,'), (' t(', ' t(m_t,-1,') ,(' s(', 's(m_s,-1/3,'), (' tt(', ' tt(m_tt,-1,'), (' c(', 'c(m_c,2/3,'),(' b(', 'b(m_b,-1/3,'), ('Anti ', 'Anti,'), ('Off ', 'Off,'), ('  ', ' ') ):\n",
    "        data = data.replace(*r)\n",
    "\n",
    "    return data\n",
    "\n",
    "#preprocessing for the squared amplitudes:\n",
    "def prepro_squared_ampl(data):\n",
    "\n",
    "    for r in (('*', '*'), (',', ' , '), ('*(', ' *( ') , ('([', '[ '), ('])', ' ]'), ('[', '[ '), (']', ' ]'), ('[ start ]', '[start]'), ('[ end ]', '[end]'), (' - ', ' -'), (' + ',' +' ) ,('/', ' / ') ,('  ', ' ')) :\n",
    "        data = data.replace(*r)\n",
    "    data = re.sub(r\"\\*(s_\\d+\\*s_\\d+)\", r\"* \\1\", data)\n",
    "    data = re.sub(r\"\\*(s_\\d+\\^\\d+\\*s_\\d+)\", r\"* \\1\", data)\n",
    "    data = re.sub(r\"\\*(m_\\w+\\^\\d+\\*s_\\d+)\", r\"* \\1\", data)\n",
    "    data = re.sub(r\"(m_\\w+\\^\\d+)\", r\" \\1 \", data)\n",
    "    data = data.replace('  ', ' ')\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def max_len(sq_data):\n",
    "    l = len(sq_data[sq_data.index(max(sq_data, key=len))].split())\n",
    "    return l\n",
    "\n",
    "\n",
    "\n",
    "with open(\"DATA/fey_qed_order.txt\", 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "text_pairs =[]\n",
    "\n",
    "for line in lines[: min(len(lines), len(lines)-1)]:\n",
    "    intr, amp, sqamp, t  = line.split('>')\n",
    "    #sqamp = \"[start] \" + sqamp + \" [end]\"\n",
    "    text_pairs.append((intr, amp,sqamp, float(t) ))\n",
    "\n",
    "text_pairs = list(set(text_pairs))\n",
    "print('data size: ', len(text_pairs))\n",
    "\n",
    "text_pairs_prep = []\n",
    "for i in range(100):\n",
    "    text_pairs_prep.append((text_pairs[i][0], prepro_feyn(text_pairs[i][1]),prepro_squared_ampl(text_pairs[i][2] ) , text_pairs[i][3]))\n",
    "\n",
    "text_pairs = text_pairs_prep\n",
    "\n",
    "feyn = [pair[1] for pair in text_pairs]\n",
    "sq_ampl= [pair[2] for pair in text_pairs]\n",
    "\n",
    "print( 'Maximum sequence length of Feynman diagram        :' ,max_len(feyn))\n",
    "print( 'Maximum sequence length of squared amplitudes:' ,max_len(sq_ampl))\n",
    "print('Example sqamp:' , sq_ampl[1])\n",
    "string=\"\"\n",
    "for i in sq_ampl[1]:\n",
    "     string+=i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": './DATA/fey_qed_order.txt',\n",
    "        \"lang_src\": 1,\n",
    "        \"lang_tgt\": 2,\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "ds_raw =text_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    #for item in ds:\n",
    "    #   yield item[lang]\n",
    "    for text  in ds:\n",
    "        for token in text[lang]:\n",
    "            yield token.split()\n",
    "\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    # giving a language it creates the  config['tokenizer_file']='../tokenizers/tokenizer_{0}.json'\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size source:  33\n",
      "Vocab size target:  {'b': 28, '/': 32, 's': 5, '3': 11, '5': 10, '[EOS]': 3, '4': 8, '-': 12, '[': 19, 'c': 29, ']': 20, '[SOS]': 2, '(': 17, '8': 23, '*': 6, '1': 9, 't': 21, 'e': 26, 'd': 27, '2': 7, '6': 22, '^': 14, 'm': 15, '7': 30, '[PAD]': 1, '0': 31, '9': 25, '_': 4, '+': 13, ',': 16, ')': 18, 'u': 24, '[UNK]': 0}\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size source: ',tokenizer_src.get_vocab_size())\n",
    "print('Vocab size target: ',tokenizer_tgt.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "\n",
    "train_ds_size = int(0.9 * len(ds_raw))\n",
    "val_ds_size = len(ds_raw) - train_ds_size\n",
    "train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x22ef1df2490>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
